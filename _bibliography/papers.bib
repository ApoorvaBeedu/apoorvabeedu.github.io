---
---
@misc{beedu2024efficacy,
      title={On the Efficacy of Text-Based Input Modalities for Action Anticipation}, 
      author={Apoorva Beedu and Karan Samel and Irfan Essa},
      year={2024},
      eprint={2401.12972},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      selected={true},
      pdf={MAT.pdf},
  abstract={Although the task of anticipating future actions is highly uncertain, information from additional modalities help to narrow down plausible action choices. Each modality provides different environmental context for the model to learn from. While previous multi-modal methods leverage information from modalities such as video and audio, we primarily explore how text inputs for actions and objects can also enable more accurate action anticipation. Therefore, we propose a Multi-modal Anticipative Transformer (MAT), an attention-based video transformer architecture that jointly learns from multi-modal features and text captions. We train our model in two-stages, where the model first learns to predict actions in the video clip by aligning with captions, and during the second stage, we fine-tune the model to predict future actions. Compared to existing methods, MAT has the advantage of learning additional environmental context from two kinds of text inputs: action descriptions during the pre-training stage, and the text inputs for detected objects and actions during modality feature fusion. Through extensive experiments, we evaluate the effectiveness of the pre-training stage, and show that our model outperforms previous methods on all datasets. In addition, we examine the impact of object and action information obtained via text and perform extensive ablations. We evaluate the performance on on three datasets: EpicKitchens-100, EpicKitchens-55 and EGTEA GAZE+; and show that text descriptions do indeed aid in more effective action anticipation.}
}
@article{choi2023multimodal,
  title={Multimodal contrastive learning with hard negative sampling for human activity recognition},
  author={Choi, Hyeongju and Beedu, Apoorva and Essa, Irfan},
  journal={ICCV 2023 workshop on PerDream: PERception, Decision making and REAsoning through Multimodal foundational modeling},
  year={2023},
  selected={true},
  pdf={Multimodal Contrastive Learning.pdf},
  abstract={Human Activity Recognition (HAR) systems have been extensively studied by the vision and ubiquitous computing communities due to their practical applications in daily life, such as smart homes, surveillance, and health monitoring. Typically, this process is supervised in nature and the development of such systems requires access to large quantities of annotated data. However, the higher costs and challenges associated with obtaining good quality annotations have rendered the application of self-supervised methods an attractive option and contrastive learning comprises one such method. However, a major component of successful contrastive learning is the selection of good positive and negative samples. Although positive samples are directly obtainable, sampling good negative samples remain a challenge. As human activities can be recorded by several modalities like camera and IMU sensors, we propose a hard negative sampling method for multimodal HAR with a hard negative sampling loss for skeleton and IMU data pairs. We exploit hard negatives that have different labels from the anchor but are projected nearby in the latent space using an adjustable concentration parameter. Through extensive experiments on two benchmark datasets: UTD-MHAD and MMAct, we demonstrate the robustness of our approach forlearning strong feature representation for HAR tasks, and on the limited data setting. We further show that our model outperforms all other state-of-the-art methods for UTD-MHAD dataset, and self-supervised methods for MMAct: Cross session, even when uni-modal data are used during downstream activity recognition.}
}

@article{choi2022multi,
  title={Multi-Stage Based Feature Fusion of Multi-Modal Data for Human Activity Recognition},
  author={Choi, Hyeongju and Beedu, Apoorva and Haresamudram, Harish and Essa, Irfan},
  journal={arXiv preprint arXiv:2211.04331},
  year={2022},
  pdf={Multi_stage.pdf},
  abstract={To properly assist humans in their needs, human activity recognition (HAR) systems need the ability to fuse information from multiple modalities. Our hypothesis is that multimodal sensors, visual and non-visual tend to provide complementary information, addressing the limitations of other modalities. In this work, we propose a multi-modal framework that learns to effectively combine features from RGB Video and IMU sensors, and show its robustness for MMAct and UTD-MHAD datasets. Our model is trained in two-stage, where in the first stage, each input encoder learns to effectively extract features, and in the second stage, learns to combine these individual features. We show significant improvements of 22% and 11% compared to video only and IMU only setup on UTD-MHAD dataset, and 20% and 12% on MMAct datasets. Through extensive experimentation, we show the robustness of our model on zero shot setting, and limited annotated data setting. We further compare with state-of-the-art methods that use more input modalities and show that our method outperforms significantly on the more difficult MMact dataset, and performs comparably in UTD-MHAD dataset.}
}

@article{beedu2022video,
  title={Video based Object 6D Pose Estimation using Transformers},
  author={Beedu, Apoorva and Alamri, Huda and Essa, Irfan},
  journal={NeurIPS 2022 workshop on Vision Transformers: Theory and Applications},
  year={2022},
    booktitle={},
      selected={true},
      pdf={VideoPose_Transformer.pdf},
      abstract={We introduce a Transformer based 6D Object Pose Estimation framework VideoPose, comprising an end-to-end attention based modelling architecture, that attends to previous frames in order to estimate accurate 6D Object Poses in videos. Our approach leverages the temporal information from a video sequence for pose refinement, along with being computationally efficient and robust. Compared to existing methods, our architecture is able to capture and reason from long-range dependencies efficiently, thus iteratively refining over video sequences. Experimental evaluation on the YCB-Video dataset shows that our approach is on par with the state-of-the-art Transformer methods, and performs significantly better relative to CNN based approaches. Further, with a speed of 33 fps, it is also more efficient and therefore applicable to a variety of applications that require real-time object pose estimation. Training code and pretrained models are available at https://github.com/ApoorvaBeedu/VideoPose.},
}

@misc{beedu2022AVSDTransformer,
  doi = {10.48550/ARXIV.2210.14512},
  url = {https://arxiv.org/abs/2210.14512},
  author = {Alamri, Huda and Bilic, Anthony and Hu, Michael and Beedu, Apoorva and Essa, Irfan},
  journal={NeurIPS 2022 workshop on Vision Transformers: Theory and Applications},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {End-to-End Multimodal Representation Learning for Video Dialog},
  year = {2022}, 
  copyright = {Creative Commons Attribution 4.0 International},
  selected={true},
  pdf={AVSD_Transformer.pdf},
  abstract={Video-based dialog task is a challenging multimodal learning task that has received increasing attention over the past few years with state-of-the-art obtaining new performance records. This progress is largely powered by the adaptation of the more powerful transformer-based language encoders. Despite this progress, existing approaches do not effectively utilize visual features to help solve tasks. Recent studies show that state-of-the-art models are biased toward textual information rather than visual cues. In order to better leverage the available visual information, this study proposes a new framework that combines 3D-CNN network and transformer-based networks into a single visual encoder to extract more robust semantic representations from videos. The visual encoder is jointly trained end-to-end with other input modalities such as text and audio. Experiments on the AVSD task show significant improvement over baselines in both generative and retrieval tasks. }
}

@article{beedu2021videopose,
  title={VideoPose: Estimating 6D object pose from videos},
  author={Beedu, Apoorva and Ren, Zhile and Agrawal, Varun and Essa, Irfan},
  journal={arXiv preprint arXiv:2111.10677},
  year={2021},
  selected={false},
      pdf={VideoPose.pdf},
      abstract={We introduce a simple yet effective algorithm that uses convolutional neural networks to directly estimate object poses from videos. Our approach leverages the temporal information from a video sequence, and is computationally efficient and robust to support robotic and AR domains. Our proposed network takes a pre-trained 2D object detector as input, and aggregates visual features through a recurrent neural network to make predictions at each frame. Experimental evaluation on the YCB-Video dataset show that our approach is on par with the state-of-the-art algorithms. Further, with a speed of 30 fps, it is also more efficient than the state-of-the-art, and therefore applicable to a variety of applications that require real-time object pose estimation.},
}


@inproceedings{haresamudram2020masked,
  title={Masked reconstruction based self-supervision for human activity recognition},
  author={Haresamudram, Harish and Beedu, Apoorva and Agrawal, Varun and Grady, Patrick L and Essa, Irfan and Hoffman, Judy and Pl{\"o}tz, Thomas},
  booktitle={Proceedings of the 2020 International Symposium on Wearable Computers},
  pages={45--49},
  year={2020},
  pdf={masked_reconstruction.pdf},
  selected={true},
  abstract={The ubiquitous availability of wearable sensing devices has rendered large scale collection of movement data a straightforward endeavor. Yet, annotation of these data remains a challenge and as such, publicly available datasets for human activity recognition (HAR) are typically limited in size as well as in variability, which constrains HAR model training and effectiveness. We introduce masked reconstruction as a viable self-supervised pre-training objective for human activity recognition and explore its effectiveness in comparison to state-of-the-art unsupervised learning techniques. In scenarios with small labeled datasets, the pre-training results in improvements over end-to-end learning on two of the four benchmark datasets. This is promising because the pre-training objective can be integrated “as is” into state-of-the-art recognition pipelines to effectively facilitate improved model robustness, and thus, ultimately, leading to better recognition performance.}
}

@inproceedings{apoorva2015location,
  title={Location based payload imaging},
  author={Apoorva, J and Mohan, Brinda and Beedu, Apoorva and Nayak, Mahendra M and Rao, Divya and Agrawal, VK},
  booktitle={2015 IEEE International Conference on Electronics, Computing and Communication Technologies (CONECCT)},
  pages={1--6},
  year={2015},
  organization={IEEE},
  pdf={location_based_payload_imaging.pdf},
  abstract={PISAT is a nano-satellite currently under development at PES University, Bangalore. It is an imaging satellite with the GOMSPACE Nanocam C1U as its main payload. It is three axis stabilized with active magnetic control system. Data reception and transmission is through S-band communication system. PES ground station has been commissioned exclusively for the purpose of communicating with PISAT. In the present configuration of PISAT, imaging can be carried out only during ground station visibility, which is for approximately 15 minutes. The dedicated ground station being located at Bangalore, the satellite can thus capture images only over Bangalore in this current mode. However, it is desirable that PISAT be able to carry out imaging at any commanded latitude and longitude. This paper presents a method to add this capability of imaging anywhere by including a provision to estimate the time required to reach the desired latitude and longitude using Location-Based Payload Imaging.}

}
