---
---



@misc{beedu2021videopose,
      title={VideoPose: Estimating 6D object pose from videos}, 
      author={Apoorva Beedu and Zhile Ren and Varun Agrawal and Irfan Essa},
      year={2021},
      eprint={2111.10677},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      selected={true},
      pdf={VideoPose.pdf},
      abstract={We introduce a simple yet effective algorithm that uses convolutional neural networks to directly estimate object poses from videos. Our approach leverages the temporal information from a video sequence, and is computationally efficient and robust to support robotic and AR domains. Our proposed network takes a pre-trained 2D object detector as input, and aggregates visual features through a recurrent neural network to make predictions at each frame. Experimental evaluation on the YCB-Video dataset show that our approach is on par with the state-of-the-art algorithms. Further, with a speed of 30 fps, it is also more efficient than the state-of-the-art, and therefore applicable to a variety of applications that require real-time object pose estimation.}
}


@inproceedings{haresamudram2020masked,
  title={Masked reconstruction based self-supervision for human activity recognition},
  author={Haresamudram, Harish and Beedu, Apoorva and Agrawal, Varun and Grady, Patrick L and Essa, Irfan and Hoffman, Judy and Pl{\"o}tz, Thomas},
  booktitle={Proceedings of the 2020 International Symposium on Wearable Computers},
  pages={45--49},
  year={2020},
  pdf={masked_reconstruction.pdf},
  selected={true},
  abstract={The ubiquitous availability of wearable sensing devices has rendered large scale collection of movement data a straightforward endeavor. Yet, annotation of these data remains a challenge and as such, publicly available datasets for human activity recognition (HAR) are typically limited in size as well as in variability, which constrains HAR model training and effectiveness. We introduce masked reconstruction as a viable self-supervised pre-training objective for human activity recognition and explore its effectiveness in comparison to state-of-the-art unsupervised learning techniques. In scenarios with small labeled datasets, the pre-training results in improvements over end-to-end learning on two of the four benchmark datasets. This is promising because the pre-training objective can be integrated “as is” into state-of-the-art recognition pipelines to effectively facilitate improved model robustness, and thus, ultimately, leading to better recognition performance.}
}

@inproceedings{apoorva2015location,
  title={Location based payload imaging},
  author={Apoorva, J and Mohan, Brinda and Beedu, Apoorva and Nayak, Mahendra M and Rao, Divya and Agrawal, VK},
  booktitle={2015 IEEE International Conference on Electronics, Computing and Communication Technologies (CONECCT)},
  pages={1--6},
  year={2015},
  organization={IEEE},
  pdf={location_based_payload_imaging.pdf},
  abstract={PISAT is a nano-satellite currently under development at PES University, Bangalore. It is an imaging satellite with the GOMSPACE Nanocam C1U as its main payload. It is three axis stabilized with active magnetic control system. Data reception and transmission is through S-band communication system. PES ground station has been commissioned exclusively for the purpose of communicating with PISAT. In the present configuration of PISAT, imaging can be carried out only during ground station visibility, which is for approximately 15 minutes. The dedicated ground station being located at Bangalore, the satellite can thus capture images only over Bangalore in this current mode. However, it is desirable that PISAT be able to carry out imaging at any commanded latitude and longitude. This paper presents a method to add this capability of imaging anywhere by including a provision to estimate the time required to reach the desired latitude and longitude using Location-Based Payload Imaging.}

}
